{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PDFToTokens.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO33MoZu1MXyPQZu7HLU3Ir"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z9GmrBsfvwbL"},"source":["# Convert PDF to Tokens\n","This notebook contains the functions to get raw text from PDFs, process the text and tokenize the text.\n","***\n","\n","### Functions\n","\n","1.  Extract Raw Text\n","    * getText_PyPDF2(pathToFile)\n","    * getText(pathToFile)  \n","    * savePDFAsTxt(text, filename, pathToTxtFilesFolder)\n","    * preprocessText(text)\n","\n","3.  Process Text\n","    * lemmatizeText(text, nlp)\n","    * get_nlp( )  \n","    * getStopWords(pathToCommonWordsCsv, moreWords)\n","    * processText(sentence, stopWords, nlp)\n","\n","2.  Tokenize Text\n","    * getSentences(text, nlp)\n","    * getTokens(sentences, nlp, pathToCommonWordsCsv)\n","\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"FopRDshjbybf"},"source":["#### 1. Get Raw Text\n","\n","Extract text from PDF using pdfplumber or PyPDF2."]},{"cell_type":"code","metadata":{"id":"k7SpBCDhbtLz"},"source":["\"\"\"\n","Get raw text from pdf file as string using PyPDF2\n","\"\"\"\n","def getText_PyPDF2(pathToFile: str) -> str:\n","    # Load pdf file\n","    pdfFile = open(pathToFile, 'rb')\n","    PDF_Reader = PyPDF2.PdfFileReader(pdfFile)\n","\n","    # Get total number of pages in document\n","    numPages = PDF_Reader.getNumPages()\n","    #print(f\"There are {numPages} pages in the file.\\n\")\n","\n","    # Combine text from all pages into one string\n","    text = \"\"\n","    for pg_number in range(numPages):\n","      page = PDF_Reader.getPage(pg_number)\n","      page_text = page.extractText()\n","      text += page_text\n","    \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQMipJgxitDM"},"source":["\"\"\"\n","Extract raw text from pdf using pdfplumber\n","\n","Problems with PyPDF2:\n","  - only extracted 2 of 13 pages for a file\n","  - added many newline chars where they don't exist in the original \n","\n","Improvements with pdfplumber:\n","  - able to extract ALL text, including header, footer, image captions\n","  - keeps general format of original pdf, just makes it all left-aligned\n","  - all words extracted are as they appear in pdf \n","    (significantly reduced the \"fake\" words)\n","\n","Initially found here: \n","https://towardsdatascience.com/how-to-extract-text-from-pdf-245482a96de7\n","\"\"\"\n","def getText(pathToFile: str) -> str:\n","    # Open pdf file\n","    pdfFile = pdfplumber.open(pathToFile)\n","\n","    # Get list of all pages' objects\n","    allPages = pdfFile.pages\n","\n","    # Extract text from each page and store into one string\n","    allText = \"\"\n","    for pageObject in allPages:\n","        pageText = pageObject.extract_text()\n","        allText += pageText\n","    \n","    return allText"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxVWymZgAqR-"},"source":["\"\"\"\n","Save extracted text into .txt file,\n","using the same name (e.g. SanJose1.pdf => SanJose1.txt)\n","\"\"\"\n","def savePDFAsTxt(text, filename, pathToTxtFilesFolder):\n","    # Create txt file name\n","    name = filename[:-4] #remove .pdf\n","    txtFilename = name + \".txt\"\n","\n","    # Make and save txt file with name\n","    pathToCurrentTxtFile = pathToTxtFilesFolder + txtFilename\n","    txtFile = open(pathToCurrentTxtFile, 'w+')    \n","    \n","    # Write to file with string of pdf's text\n","    txtFile.write(text.strip())\n","    print(f\"Saved as {txtFilename}\")\n","    return pathToCurrentTxtFile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAruX94AiwSQ"},"source":["\"\"\"\n","Preprocess text by replacing newline with a space\n","\"\"\"\n","def preprocessText(text: str) -> str:\n","    # Make lower case, and remove newline\n","    preprocessedText = text.replace(\"\\n\", \" \")\n","    return preprocessedText"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XsAHlZoemUA8"},"source":["***\n","#### 2. Process Text\n","\n","Remove stop words and lemmatize text."]},{"cell_type":"code","metadata":{"id":"NIVnX8XN99et"},"source":["\"\"\"\n","Lemmatize string with text\n","\"\"\"\n","def lemmatizeText(text, nlp) -> str:\n","    tokens = nlp(text)\n","    lemmatizedWords = []\n","    for token in tokens:\n","        lemmatizedWords.append(token.lemma_)\n","    return lemmatizedWords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1sYwvmlBjme"},"source":["\"\"\"\n","Create spacy Language object to parse English text\n","\"\"\"\n","def get_nlp():\n","    nlp = spacy.load('en')\n","    return nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4h4cTc9CEst9"},"source":["\"\"\" \n","Get list of stop words from spacy, \n","optionally can add extra words\n","\"\"\"\n","def getStopWords(pathToCommonWordsCsv, moreWords=[]) -> list:\n","  stopWords = spacy.lang.en.stop_words.STOP_WORDS #set\n","  stopWordsList = list(stopWords) + moreWords\n","  # This uses CommonWords.csv\n","  commonWords = getCommonWords(pathToCommonWordsCsv)\n","  if len(commonWords) > 0:\n","      stopWordsList += commonWords[:15] # top 15 common words    \n","  \n","  stopWordsList += ['san', 'jose', 'josÃ©', 'city', 'council', \n","                    'meeting', 'resolution', 'memorandum',\n","                    'event', 'file', 'document', 'agenda',\n","                    'draft', 'contact', 'office', 'resource',\n","                    'clerk', 'final', 'california',]\n","\n","  return stopWordsList"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlcUcLXb-BPL"},"source":["\"\"\"\n","Processing text: lemmatize, remove stop words, make lowercase\n","Input:\n","    sentence: str, \n","    stopWords: set, \n","    nlp: spacy.lang.en.English\n","\n","Note: when the same word appeared in a sentence (e.g. Fees and fees),\n","spaCy only lemmatized fees to 'fee', but did not lemmatize Fees.\n","Some other times, it did lemmatize (e.g. Authorizes and authorizes).\n","\"\"\"\n","def processText(sentence, stopWords, nlp) -> list:\n","    # Lemmatize 2 times\n","    # Lemmatize entire sentence\n","    tokens_lemmatized = lemmatizeText(sentence, nlp)\n","    # Make each word of sentence into lower case words\n","    tokens_lowercase = [word.lower() for word in tokens_lemmatized]\n","    # Lemmatize each word to ensure they are lemmatized \n","    tokens_lemmatized = [lemmatizeText(word, nlp)[0] for word in tokens_lowercase]\n","    \n","    # Removing stop words and any punctuation or numeric strings\n","    list_tokens = []\n","    for word in tokens_lemmatized:\n","        if word not in stopWords and word.isalpha():\n","            list_tokens.append(word)\n","    \n","    # Remove single letters\n","    list_tokens = [word for word in list_tokens if len(word)>1]\n","\n","    # Return preprocessed list of tokens\n","    return list_tokens  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D18rlAdyLbN0"},"source":["***\n","#### 3. Tokenize Text\n","\n","Break text into sentences and process further by breaking it into words."]},{"cell_type":"code","metadata":{"id":"fY8FxUmM_Uk1"},"source":["\"\"\"\n","Get sentences from pdf's preprocessed text using spacy's trained pipeline\n","\"\"\"\n","def getSentences(text: str, nlp) -> list:\n","    #nlp = get_nlp() #spacy.load('en_core_web_sm')\n","    doc = nlp(text)\n","    sentences = list(doc.sents)\n","    sentences = [sent.string.strip() for sent in sentences]\n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3sOpAfE_WLA"},"source":["\"\"\"\n","Break sentences into words by processing them\n","\"\"\"\n","def getTokens(sentences, nlp, pathToCommonWordsCsv=\"\") -> list:\n","    stopWords = getStopWords(pathToCommonWordsCsv)\n","    # Process sentences to get words\n","    tokens = []\n","    for sentence in sentences:\n","        current_tokens = processText(sentence, stopWords, nlp)\n","        tokens += current_tokens\n","    # Return words with alphabet only\n","    return tokens"],"execution_count":null,"outputs":[]}]}