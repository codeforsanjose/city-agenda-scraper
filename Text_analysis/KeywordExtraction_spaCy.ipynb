{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction_spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3mTYDKyR0N8"
      },
      "source": [
        "# Keyword Extraction with TD-IDF \n",
        "# Processing with spaCy\n",
        "***\n",
        "\n",
        "## Setup\n",
        "  * Install packages\n",
        "  * Mount google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQTBoh3R2I1"
      },
      "source": [
        "### Install Packages\n",
        "* PyPDF2\n",
        "* spaCyPDFReader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SWdCRyRpbd",
        "outputId": "a9c600d1-bd96-4127-bd88-b0b4fb3a443c"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=da0633add06732c8b5452fd6a00593db758c3eb3fda8ac0881aa7aaef56188f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PzXtmAkSPU8",
        "outputId": "e2d141a1-5e35-419f-cba7-6d0af8ef2459"
      },
      "source": [
        "!pip install spacypdfreader"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement spacypdfreader (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for spacypdfreader\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoDLV33BR9_L"
      },
      "source": [
        "\n",
        "### Mount Google Drive\n",
        "* Files from two folders: Memorandums, Resolutions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-aHUrePSGOj",
        "outputId": "9226d1cb-3dc1-4486-f1ac-e4405e530687"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTDsoxg3Smo4"
      },
      "source": [
        "*** \n",
        "# Functions\n",
        "\n",
        "1.  Processing Text: load pdf text and create tokens\n",
        "    * getText(pathToFile) \n",
        "    * preprocessText(text)\n",
        "    * getStopWords( )\n",
        "    * getParser( )\n",
        "    * processText(sentence, stopWords, parser)\n",
        "    * getSentences(text)\n",
        "    * getTokens(sentences)\n",
        "\n",
        "3.  TF-IDF: compute scores and get keywords\n",
        "    * get_tf_idf(tokens)\n",
        "    * getKeywords(pathToFile)\n",
        "  \n",
        "4.  Test: compare code's keywords with correct keywords\n",
        "    * getCorrectKeywords(pathToFile)\n",
        "    * testFilename(pathToFile)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17v46NKdY5Ab"
      },
      "source": [
        "### 1. Processing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WOWJLMYebjn"
      },
      "source": [
        "Load text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TioxpRKZTwG_"
      },
      "source": [
        "\"\"\"\n",
        "Get raw text from pdf file as string using PyPDF2\n",
        "\"\"\"\n",
        "def getText(pathToFile: str) -> str:\n",
        "    # Load pdf file\n",
        "    pdfFile = open(pathToFile, 'rb')\n",
        "    PDF_Reader = PyPDF2.PdfFileReader(pdfFile)\n",
        "\n",
        "    # Get total number of pages in document\n",
        "    numPages = PDF_Reader.getNumPages()\n",
        "    #print(f\"There are {numPages} pages in the file.\\n\")\n",
        "\n",
        "    # Combine text from all pages into one string\n",
        "    text = \"\"\n",
        "    for pg_number in range(numPages):\n",
        "      page = PDF_Reader.getPage(pg_number)\n",
        "      page_text = page.extractText()\n",
        "      text += page_text\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KgqjqJVOUaSm",
        "outputId": "ac93d8b5-f976-4299-ad94-cfd0ad2e891e"
      },
      "source": [
        "\"\"\"\n",
        "Get text from pdf as a spacy doc object using spacypdfreader\n",
        "(spacypdfreader was not able to get installed)\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nGet text from pdf as a spacy doc object using spacypdfreader\\n(spacypdfreader was not able to get installed)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCilKovVZZx"
      },
      "source": [
        "Process Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk7fVH0aU8P1"
      },
      "source": [
        "\"\"\"\n",
        "Preprocess text by replacing newline with a space\n",
        "\"\"\"\n",
        "def preprocessText(text: str) -> str:\n",
        "    # lower case, and remove newline\n",
        "    preprocessedText = text.replace(\"\\n\", \" \")\n",
        "    return preprocessedText"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUILWbaRWVOU"
      },
      "source": [
        "\"\"\" \n",
        "Get list of stop words from spacy\n",
        "\"\"\"\n",
        "def getStopWords():\n",
        "  stopWords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  return stopWords"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFOOwJNRW9_b"
      },
      "source": [
        "\"\"\"\n",
        "Create spacy Language object to parse English text\n",
        "\"\"\"\n",
        "def getParser():\n",
        "    parser = English()\n",
        "    return parser"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVSY1_bVbKF"
      },
      "source": [
        "\"\"\"\n",
        "Processing text: lemmatize, remove stop words, make lowercase\n",
        "Input:\n",
        "    sentence: str, \n",
        "    stopWords: set, \n",
        "    parser: spacy.lang.en.English\n",
        "\"\"\"\n",
        "\n",
        "def processText(sentence, stopWords, parser) -> list:\n",
        "    \n",
        "    # Create token object \n",
        "    tokens = parser(sentence)\n",
        "    \n",
        "    # lemmatize each token and make them lower case\n",
        "    tokens_lemmatized = [word.lemma_ for word in tokens]\n",
        "    tokens_lowercase = [(word.lower().strip()) for word in tokens_lemmatized]\n",
        "    \n",
        "    # Removing stop words and any punctuation or numeric strings\n",
        "    list_tokens = []\n",
        "    for word in tokens_lowercase:\n",
        "        if word not in stopWords and word.isalpha():\n",
        "            list_tokens.append(word)\n",
        "    \n",
        "    # Remove single letters\n",
        "    list_tokens = [word for word in list_tokens if len(word)>1]\n",
        "    \n",
        "    # Return preprocessed list of tokens\n",
        "    return list_tokens  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUenlYFNhXya"
      },
      "source": [
        "\"\"\"\n",
        "Get sentences from pdf's preprocessed text using spacy's trained pipeline\n",
        "\"\"\"\n",
        "def getSentences(text: str) -> list:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    sentences = [sent.string.strip() for sent in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HG6t30RfdUc"
      },
      "source": [
        "\"\"\"\n",
        "Break sentences into words by processing them\n",
        "\"\"\"\n",
        "def getTokens(sentences: list) -> list:\n",
        "    stopWords = getStopWords()\n",
        "    parser = getParser()\n",
        "    # process sentences to get words\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        current_tokens = processText(sentence, stopWords, parser)\n",
        "        tokens += current_tokens\n",
        "    # return words with alphabet only\n",
        "    return tokens"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQO0773tX9pM"
      },
      "source": [
        "### 2. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJ_OOuHX99J"
      },
      "source": [
        "\"\"\"\n",
        "Run tf-idf algorithm on the list of tokens, and\n",
        "return a dataframe with tokens and scores\n",
        "\"\"\"\n",
        "def get_TF_IDF(tokens: list):\n",
        "    \n",
        "    # compute TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_vectorizer.fit(tokens)\n",
        "\n",
        "    tf_idf = list(tfidf_vectorizer.idf_) #scores\n",
        "    features = list(tfidf_vectorizer.get_feature_names()) #words/tokens\n",
        "\n",
        "    # store results in dataframe\n",
        "    scores_df = pd.DataFrame(list(zip(features, tf_idf)), \n",
        "                             columns=['Keywords', 'TF-IDF'])\n",
        "    scores_df = scores_df.sort_values('TF-IDF').reset_index(drop=True)\n",
        "    \n",
        "    return scores_df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmxP3DxjeVH"
      },
      "source": [
        "\"\"\"\n",
        "Combine all functions to get keywords dataframe from just path to file\n",
        "\"\"\"\n",
        "def getKeywords(pathToFile: str): \n",
        "  rawText = getText(pathToFile=pathToFile)\n",
        "  preprocessedText = preprocessText(text = rawText)\n",
        "  sentences = getSentences(text = preprocessedText)\n",
        "  tokens = getTokens(sentences = sentences)\n",
        "\n",
        "  # get dataframe with keywords and scores\n",
        "  keywords_df = get_TF_IDF(tokens)\n",
        "\n",
        "  return keywords_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpRNQmKJl5Oa"
      },
      "source": [
        "### 3. Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEh8poeIluBG"
      },
      "source": [
        "\"\"\"\n",
        "Extract words from filename, \n",
        "which are separated by an underscore _\n",
        "\"\"\"\n",
        "def getCorrectKeywords(pathToFile: str) -> list:\n",
        "    filename = pathToFile.split('/')[-1]\n",
        "    correctKeywords = filename.replace(\".pdf\", \"\").split('_')\n",
        "    correctKeywords = [word.lower() for word in correctKeywords]\n",
        "    return correctKeywords"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7xX3Z0Il7KD"
      },
      "source": [
        "\"\"\"\n",
        "Test one file\n",
        "Compare correct keywords with the top 10 keywords computed with tf-idf\n",
        "\"\"\"\n",
        "def testFilename(pathToFile: str) -> float:\n",
        "    # get top 10 keywords using tf-idf\n",
        "    tf_idf_keywords = getKeywords(pathToFile)['Keywords'].to_list()[:10]\n",
        "\n",
        "    # get actual keywords from file name\n",
        "    correctKeywords = getCorrectKeywords(pathToFile)\n",
        "\n",
        "    numCorrectWordsFound = 0\n",
        "    for keyword in correctKeywords:\n",
        "        if keyword in tf_idf_keywords:\n",
        "            #print(f\"{keyword} was found.\")\n",
        "            numCorrectWordsFound += 1\n",
        "            \n",
        "        #else:\n",
        "            #print(f\"The word '{keyword}' was not found in tf-idf keywords.\")\n",
        "    \n",
        "    correctPercentage = round((numCorrectWordsFound/len(correctKeywords))*100, 2)\n",
        "    print(f\"{correctPercentage}% of keywords were found.\")\n",
        "\n",
        "    return correctPercentage"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3YghvSSX_c"
      },
      "source": [
        "*** \n",
        "# Run Tests\n",
        "  1. Load libraries\n",
        "  2. Test 1 File\n",
        "  3. Test All Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgCU_9LhWBjU"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "import PyPDF2\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaFSekRa3FAI"
      },
      "source": [
        "### Test One File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzdGJjg5i7lw"
      },
      "source": [
        "Memorandum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ql4p9Wg8ac3i",
        "outputId": "74343d48-280d-41ba-e3fe-f1f4e6bd1487"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "filename = \"Historic_Landmarks_Designation_Property.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75.0% of keywords were found.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>commission</td>\n",
              "      <td>4.258097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>city</td>\n",
              "      <td>4.258097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>august</td>\n",
              "      <td>4.258097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>director</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resolution</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>property</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>chris</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>burton</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>landmarks</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>historic</td>\n",
              "      <td>4.545779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Keywords    TF-IDF\n",
              "0  commission  4.258097\n",
              "1        city  4.258097\n",
              "2      august  4.258097\n",
              "3    director  4.545779\n",
              "4  resolution  4.545779\n",
              "5    property  4.545779\n",
              "6       chris  4.545779\n",
              "7      burton  4.545779\n",
              "8   landmarks  4.545779\n",
              "9    historic  4.545779"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk2u8-yNi_gX"
      },
      "source": [
        "Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "USCtTBGei-w6",
        "outputId": "060f0948-203c-4f46-8b18-793558bd8017"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "filename = \"Fire_Department_Exam_Free_Use_Hall.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.0% of keywords were found.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>city</td>\n",
              "      <td>3.429477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fire</td>\n",
              "      <td>4.183249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>council</td>\n",
              "      <td>4.316780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ﬁfree</td>\n",
              "      <td>4.470931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>useﬂ</td>\n",
              "      <td>4.470931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>saturday</td>\n",
              "      <td>4.470931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>department</td>\n",
              "      <td>4.470931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>resolution</td>\n",
              "      <td>4.653252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>august</td>\n",
              "      <td>4.653252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>hall</td>\n",
              "      <td>4.653252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Keywords    TF-IDF\n",
              "0        city  3.429477\n",
              "1        fire  4.183249\n",
              "2     council  4.316780\n",
              "3       ﬁfree  4.470931\n",
              "4        useﬂ  4.470931\n",
              "5    saturday  4.470931\n",
              "6  department  4.470931\n",
              "7  resolution  4.653252\n",
              "8      august  4.653252\n",
              "9        hall  4.653252"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyPf5yEvreqQ"
      },
      "source": [
        "### Test All Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J20LlPEHgZ3G"
      },
      "source": [
        "Memorandums"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElNUZ6Py5zGS",
        "outputId": "834a278a-c15f-4040-db6e-4c96684d02c5"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Downtown_Rezone_Addendum_Environmental.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Chief_Police_Questions_Policy_Selection.pdf\n",
            "80.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Dumpster_Day_Brooktree_Vinci_Flickinger.pdf\n",
            "40.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 4: Juneteenth_Holiday.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 5: Marriott_Townplace_Suites_Hotel_Vesting_Development_Permit.pdf\n",
            "14.29% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 6: Demolition_Permit_Site_Development_Construction_Building.pdf\n",
            "33.33% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 7: Audit_Peer_Review.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 8: District_2_Great_Oaks_Movie_Night_Coffee_Chief.pdf\n",
            "12.5% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 9: Flag_Raising_University_San_Jose_State.pdf\n",
            "16.67% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 10: Pride_Flag_Silicon_Valley.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 11: Santos_Family_Car_Show.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 12: Vaccine_Mandate_Vaccination.pdf\n",
            "33.33% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 13: Trimble_Agnews_Groundwater_Wells_Project.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 14: Historic_Landmarks_Designation_Property.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QpdvTK1gb_q"
      },
      "source": [
        "Resolutions\n",
        "\n",
        "* Was not able to get text from \"San_Carlos_Environmental_Mixed_Use.pdf\" (or SanJose16.pdf) with PyPDF2\n",
        "* just shows many newline chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QfNm0k4gdCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc75f6ea-9fe8-456c-8b5e-877cb41c7a2e"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Financing_Commercial.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Fire_Department_Exam_Free_Use_Hall.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Vacate_Almaden_Property_Surplus.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}