{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction_spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3mTYDKyR0N8"
      },
      "source": [
        "# Keyword Extraction with spaCy Processing\n",
        "***\n",
        "### Table of Contents \n",
        "\n",
        "  1. [Setup](#setup)\n",
        "\n",
        "  2. [Functions](#createfunctions)\n",
        "\n",
        "  3. [Run Tests](#run-tests)\n",
        "\n",
        "  4. [Save as Txt Files](#save-as-txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knTe5Ol48jL"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "***\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQTBoh3R2I1"
      },
      "source": [
        "### Install Packages\n",
        "* PyPDF2\n",
        "* pdfplumber\n",
        "* spaCyPDFReader (cannot install)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SWdCRyRpbd",
        "outputId": "099ceb9a-7bc1-4206-e694-2eea64e7b9fe"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=171d4b76602129a7d0831992f2920a925dc22440b9c185936ee8543655949e37\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfcHT0RRZKhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e798e3-8f54-4251-e9d8-5b94d257cfd3"
      },
      "source": [
        "!pip install pdfplumber"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.5.28.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20200517\n",
            "  Downloading pdfminer.six-20200517-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Collecting Wand\n",
            "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 43.4 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Building wheels for collected packages: pdfplumber\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.28-py3-none-any.whl size=32240 sha256=e39d2a3bb20620c648f6b40f066a46b456b3c7244466e6fbe31c9a3acc0410d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b1/a0/c0a77b756d580f53b3806ae0e0b3ec945a8d05fca1d6e10cc1\n",
            "Successfully built pdfplumber\n",
            "Installing collected packages: pycryptodome, Wand, pdfminer.six, pdfplumber\n",
            "Successfully installed Wand-0.6.7 pdfminer.six-20200517 pdfplumber-0.5.28 pycryptodome-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PzXtmAkSPU8",
        "outputId": "929919fb-5252-459b-c549-ec2d51d45c82"
      },
      "source": [
        "!pip install spacypdfreader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement spacypdfreader (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for spacypdfreader\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoDLV33BR9_L"
      },
      "source": [
        "\n",
        "### Mount Google Drive\n",
        "* Files from three folders: Memorandums, Resolutions, SanJoseFiles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-aHUrePSGOj",
        "outputId": "b286b191-a57f-49b8-bfc9-e9f7ba1c7d35"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTDsoxg3Smo4"
      },
      "source": [
        "<a name=\"createfunctions\"></a>\n",
        "*** \n",
        "# Functions\n",
        "\n",
        "1.  Preprocessing Text: load pdf raw text\n",
        "    * getText_PyPDF2(pathToFile)\n",
        "    * getText(pathToFile)  **<---new**\n",
        "    * preprocessText(text)\n",
        "\n",
        "\n",
        "2.  Get Stop Words: find common words and stop words\n",
        "    * get_nlp( )  **<---updated**\n",
        "    * getListKeywords(pathToFile)  **<---new**\n",
        "    * updateDict(words, wordsDict) **<---new**\n",
        "    * findFrequentWords( )  **<---new**\n",
        "    * saveCommonWords(pathBeforeFile)  **<---new**\n",
        "    * getCommonWords( )  **<---new**\n",
        "    * getStopWords(words) **<---updated**\n",
        "\n",
        "\n",
        "3.  Processing Text: lemmatize and tokenize text\n",
        "    * lemmatizeText(text, nlp)  **<---new**\n",
        "    * processText(sentence, stopWords, nlp)\n",
        "    * getSentences(text, nlp)\n",
        "    * getTokens(sentences, nlp)\n",
        "\n",
        "\n",
        "4.  TF-IDF: compute scores and get keywords\n",
        "    * get_tf_idf(tokens)\n",
        "    * getKeywords(pathToFile)\n",
        "  \n",
        "\n",
        "5.  Test: compare code's keywords with correct keywords\n",
        "    * getCorrectKeywords(pathToFile)\n",
        "    * testFilename(pathToFile)\n",
        "\n",
        "\n",
        "6. Save Files: save pdfs as .txt, save keywords in .csv, save common words  **<---new**\n",
        "    * saveAsTxt(filename, text)  **<---new**\n",
        "    * saveKeywordsAsCsv(filename, keywords_df) **<---new**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17v46NKdY5Ab"
      },
      "source": [
        "### 1. Preprocessing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WOWJLMYebjn"
      },
      "source": [
        "Load text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TioxpRKZTwG_"
      },
      "source": [
        "\"\"\"\n",
        "Get raw text from pdf file as string using PyPDF2\n",
        "\"\"\"\n",
        "def getText_PyPDF2(pathToFile: str) -> str:\n",
        "    # Load pdf file\n",
        "    pdfFile = open(pathToFile, 'rb')\n",
        "    PDF_Reader = PyPDF2.PdfFileReader(pdfFile)\n",
        "\n",
        "    # Get total number of pages in document\n",
        "    numPages = PDF_Reader.getNumPages()\n",
        "    #print(f\"There are {numPages} pages in the file.\\n\")\n",
        "\n",
        "    # Combine text from all pages into one string\n",
        "    text = \"\"\n",
        "    for pg_number in range(numPages):\n",
        "      page = PDF_Reader.getPage(pg_number)\n",
        "      page_text = page.extractText()\n",
        "      text += page_text\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKvrFppdZbfm"
      },
      "source": [
        "\"\"\"\n",
        "Extract raw text from pdf using pdfplumber\n",
        "\n",
        "Problems with PyPDF2:\n",
        "  - only extracted 2 of 13 pages for a file\n",
        "  - added many newline chars where they don't exist in the original \n",
        "\n",
        "Improvements with pdfplumber:\n",
        "  - able to extract ALL text, including header, footer, image captions\n",
        "  - keeps general format of original pdf, just makes it all left-aligned\n",
        "  - all words extracted are as they appear in pdf \n",
        "    (significantly reduced the \"fake\" words)\n",
        "\n",
        "Initially found here: \n",
        "https://towardsdatascience.com/how-to-extract-text-from-pdf-245482a96de7\n",
        "\"\"\"\n",
        "def getText(pathToFile: str) -> str:\n",
        "    # Open pdf file\n",
        "    pdfFile = pdfplumber.open(pathToFile)\n",
        "\n",
        "    # Get list of all pages' objects\n",
        "    allPages = pdfFile.pages\n",
        "\n",
        "    # Extract text from each page and store into one string\n",
        "    allText = \"\"\n",
        "    for pageObject in allPages:\n",
        "        pageText = pageObject.extract_text()\n",
        "        allText += pageText\n",
        "    \n",
        "    return allText"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KgqjqJVOUaSm",
        "outputId": "598b5873-7c19-4556-ad63-6d72a6b06482"
      },
      "source": [
        "\"\"\"\n",
        "Get text from pdf as a spacy doc object using spacypdfreader\n",
        "(spacypdfreader was not able to get installed)\n",
        "\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nGet text from pdf as a spacy doc object using spacypdfreader\\n(spacypdfreader was not able to get installed)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk7fVH0aU8P1"
      },
      "source": [
        "\"\"\"\n",
        "Preprocess text by replacing newline with a space\n",
        "\"\"\"\n",
        "def preprocessText(text: str) -> str:\n",
        "    # Make lower case, and remove newline\n",
        "    preprocessedText = text.replace(\"\\n\", \" \")\n",
        "    return preprocessedText"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_2VMB41BMoD"
      },
      "source": [
        "### 2. Get Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrphfiDZheqm"
      },
      "source": [
        "\"\"\"\n",
        "Create spacy Language object to parse English text\n",
        "\"\"\"\n",
        "def get_nlp():\n",
        "    nlp = spacy.load('en')\n",
        "    return nlp"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv5Cie8UBVAl"
      },
      "source": [
        "\"\"\"\n",
        "Get list of words from keywords csv\n",
        "\"\"\"\n",
        "def getListKeywords(pathToFile: str) -> list:\n",
        "    df = pd.read_csv(pathToFile)\n",
        "    words = list(df['Keywords'])\n",
        "    return words"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ2OHBhgBU5L"
      },
      "source": [
        "\"\"\"\n",
        "Update frequency of words in hash map\n",
        "\"\"\"\n",
        "def updateDict(words, wordsDict):\n",
        "    for word in words:\n",
        "        if word not in wordsDict:\n",
        "            wordsDict[word] = 1 #add word\n",
        "        else: # if word is in dict\n",
        "            wordsDict[word] += 1 #increase frequency\n",
        "    return wordsDict"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex4a904pBUul"
      },
      "source": [
        "\"\"\"\n",
        "Find the most common words in all SanJose# files\n",
        "\"\"\"\n",
        "def findFrequentWords():\n",
        "    # Path to keywords files\n",
        "    pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Keywords-spaCy/\"\n",
        "    # Names of all files of keywords csv\n",
        "    files = os.listdir(pathBeforeFile)\n",
        "\n",
        "    # Count frequency of words across all files\n",
        "    wordsDict = {} # hash map with key=word, value=frequency\n",
        "    for filename in files:\n",
        "        # Get all words in one file\n",
        "        pathToFile = pathBeforeFile + filename\n",
        "        words = getListKeywords(pathToFile)\n",
        "        wordsDict = updateDict(words, wordsDict)\n",
        "    \n",
        "    # Create dataframe from dict\n",
        "    wordsFrequency = pd.DataFrame(wordsDict.items(), \n",
        "                                  columns=['Word', 'Frequency'])\n",
        "    wordsFrequency = wordsFrequency.sort_values('Frequency', ascending=False)\n",
        "    wordsFrequency = wordsFrequency.reset_index(drop=True)\n",
        "    \n",
        "    return wordsFrequency"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZzLlwgiY2h1"
      },
      "source": [
        "\"\"\"\n",
        "Save file with words and their frequency\n",
        "\"\"\"\n",
        "def saveCommonWords(pathBeforeFile):\n",
        "    filename = \"CommonWords.csv\"\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    df_commonWords = findFrequentWords()\n",
        "    # Save df as .csv file\n",
        "    df_commonWords.to_csv(pathToFile, index=False)\n",
        "    print(f\"Saved as {filename}\")\n",
        "    return df_commonWords"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSaH7JICVKqm"
      },
      "source": [
        "\"\"\"\n",
        "Get list of words in descending order of frequency (more to less)\n",
        "\"\"\"\n",
        "def getCommonWords():\n",
        "    pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/StopWords/\"\n",
        "    filename = \"CommonWords.csv\"\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    try: \n",
        "        df = pd.read_csv(pathToFile)\n",
        "        commonWords = list(df['Word'])\n",
        "        return commonWords\n",
        "    except: \n",
        "        print(\"Could not find file for common words.\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUILWbaRWVOU"
      },
      "source": [
        "\"\"\" \n",
        "Get list of stop words from spacy, \n",
        "optionally can add extra words\n",
        "\"\"\"\n",
        "def getStopWords(words=[]) -> list:\n",
        "  stopWords = spacy.lang.en.stop_words.STOP_WORDS #set\n",
        "  # Will update/change the common words later\n",
        "  #commonWords = getCommonWords()[:15] #top 15 common words\n",
        "\n",
        "  stopWordsList = list(stopWords) + words #+ commonWords\n",
        "  \n",
        "  stopWordsList += ['san', 'jose', 'josé', 'city', 'council', \n",
        "                    'meeting', 'resolution', 'memorandum',\n",
        "                    'event', 'file', 'document', 'agenda',\n",
        "                    'draft', 'contact', 'office', 'resource',\n",
        "                    'clerk', 'final', 'california',]\n",
        "  return stopWordsList"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCilKovVZZx"
      },
      "source": [
        "### 3. Processing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eG-JKGpOmyt"
      },
      "source": [
        "\"\"\"\n",
        "Lemmatize string with text\n",
        "\"\"\"\n",
        "def lemmatizeText(text, nlp) -> str:\n",
        "    tokens = nlp(text)\n",
        "    lemmatizedWords = []\n",
        "    for token in tokens:\n",
        "        lemmatizedWords.append(token.lemma_)\n",
        "    return lemmatizedWords"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVSY1_bVbKF"
      },
      "source": [
        "\"\"\"\n",
        "Processing text: lemmatize, remove stop words, make lowercase\n",
        "Input:\n",
        "    sentence: str, \n",
        "    stopWords: set, \n",
        "    nlp: spacy.lang.en.English\n",
        "\n",
        "Note: when the same word appeared in a sentence (e.g. Fees and fees),\n",
        "spaCy only lemmatized fees to 'fee', but did not lemmatize Fees.\n",
        "Some other times, it did lemmatize (e.g. Authorizes and authorizes).\n",
        "\"\"\"\n",
        "def processText(sentence, stopWords, nlp) -> list:\n",
        "    # Lemmatize 2 times\n",
        "    # Lemmatize entire sentence\n",
        "    tokens_lemmatized = lemmatizeText(sentence, nlp)\n",
        "    # Make each word of sentence into lower case words\n",
        "    tokens_lowercase = [word.lower() for word in tokens_lemmatized]\n",
        "    # Lemmatize each word to ensure they are lemmatized \n",
        "    tokens_lemmatized = [lemmatizeText(word, nlp)[0] for word in tokens_lowercase]\n",
        "    \n",
        "    # Removing stop words and any punctuation or numeric strings\n",
        "    list_tokens = []\n",
        "    for word in tokens_lemmatized:\n",
        "        if word not in stopWords and word.isalpha():\n",
        "            list_tokens.append(word)\n",
        "    \n",
        "    # Remove single letters\n",
        "    list_tokens = [word for word in list_tokens if len(word)>1]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return list_tokens  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUenlYFNhXya"
      },
      "source": [
        "\"\"\"\n",
        "Get sentences from pdf's preprocessed text using spacy's trained pipeline\n",
        "\"\"\"\n",
        "def getSentences(text: str, nlp) -> list:\n",
        "    #nlp = get_nlp() #spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    sentences = [sent.string.strip() for sent in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HG6t30RfdUc"
      },
      "source": [
        "\"\"\"\n",
        "Break sentences into words by processing them\n",
        "\"\"\"\n",
        "def getTokens(sentences: list, nlp) -> list:\n",
        "    stopWords = getStopWords()\n",
        "    # Process sentences to get words\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        current_tokens = processText(sentence, stopWords, nlp)\n",
        "        tokens += current_tokens\n",
        "    # Return words with alphabet only\n",
        "    return tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQO0773tX9pM"
      },
      "source": [
        "### 4. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJ_OOuHX99J"
      },
      "source": [
        "\"\"\"\n",
        "Run tf-idf algorithm on the list of tokens, and\n",
        "return a dataframe with tokens and scores\n",
        "\"\"\"\n",
        "def get_TF_IDF(tokens: list):\n",
        "    # Compute TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_vectorizer.fit(tokens)\n",
        "\n",
        "    tf_idf = list(tfidf_vectorizer.idf_) #scores\n",
        "    features = list(tfidf_vectorizer.get_feature_names()) #words/tokens\n",
        "\n",
        "    # Store results in dataframe\n",
        "    scores_df = pd.DataFrame(list(zip(features, tf_idf)), \n",
        "                             columns=['Keywords', 'TF-IDF'])\n",
        "    # Sort by score in ascending - small to large - order\n",
        "    scores_df = scores_df.sort_values('TF-IDF').reset_index(drop=True)\n",
        "    \n",
        "    return scores_df"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmxP3DxjeVH"
      },
      "source": [
        "\"\"\"\n",
        "Combine all functions to get keywords dataframe from just path to file\n",
        "\"\"\"\n",
        "def getKeywords(pathToFile: str): \n",
        "  # Run process for extracting keywords\n",
        "  nlp = get_nlp()\n",
        "  rawText = getText(pathToFile)\n",
        "  preprocessedText = preprocessText(rawText)\n",
        "  sentences = getSentences(preprocessedText, nlp)\n",
        "  tokens = getTokens(sentences, nlp)\n",
        "\n",
        "  # Get dataframe with keywords and scores\n",
        "  keywords_df = get_TF_IDF(tokens)\n",
        "\n",
        "  return keywords_df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpRNQmKJl5Oa"
      },
      "source": [
        "### 5. Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEh8poeIluBG"
      },
      "source": [
        "\"\"\"\n",
        "Extract words from filename, \n",
        "which are separated by an underscore _\n",
        "\"\"\"\n",
        "def getCorrectKeywords(pathToFile: str) -> list:\n",
        "    filename = pathToFile.split('/')[-1]\n",
        "    correctKeywords = filename.replace(\".pdf\", \"\").split('_')\n",
        "    correctKeywords = [word.lower() for word in correctKeywords]\n",
        "    return correctKeywords"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7xX3Z0Il7KD"
      },
      "source": [
        "\"\"\"\n",
        "Test one file\n",
        "Compare correct keywords with the top 10 keywords computed with tf-idf\n",
        "\"\"\"\n",
        "def testFilename(pathToFile: str) -> float:\n",
        "    # Get top 10 keywords using tf-idf\n",
        "    tf_idf_keywords = getKeywords(pathToFile)['Keywords'].to_list()[:10]\n",
        "\n",
        "    # Get actual keywords from file name\n",
        "    correctKeywords = getCorrectKeywords(pathToFile)\n",
        "\n",
        "    # Count number of correct words in keywords\n",
        "    numCorrectWordsFound = 0\n",
        "    for keyword in correctKeywords:\n",
        "        if keyword in tf_idf_keywords:\n",
        "            numCorrectWordsFound += 1\n",
        "\n",
        "    # Print perfect of correct words found   \n",
        "    correctPercentage = round((numCorrectWordsFound/len(correctKeywords))*100, 2)\n",
        "    print(f\"{correctPercentage}% of keywords were found.\")\n",
        "\n",
        "    return correctPercentage"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMszvZGM7SyM"
      },
      "source": [
        "### 6. Save Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjZQb5707YvP"
      },
      "source": [
        "\"\"\"\n",
        "Save extracted text into .txt file,\n",
        "using the same name (e.g. SanJose1.pdf => SanJose1.txt)\n",
        "\"\"\"\n",
        "def savePDFAsTxt(filename: str, text: str):\n",
        "    # Create folder, if it doesn't exist \n",
        "    pathToFolder = \"/content/gdrive/My Drive/CFSJ/\"\n",
        "    os.makedirs(pathToFolder+\"TxtFiles\", exist_ok=True)\n",
        "\n",
        "    # Create txt file name\n",
        "    name = filename[:-4] #remove .pdf\n",
        "    txtFilename = name + \".txt\"\n",
        "\n",
        "    # Make txt file with name\n",
        "    txtFile = open(pathToFolder+\"TxtFiles/\"+txtFilename, 'w+')    \n",
        "    \n",
        "    # Write to file with string of pdf's text\n",
        "    txtFile.write(text.strip())\n",
        "    print(f\"Saved as {txtFilename}\")\n",
        "    return txtFile"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8dvnmP7d8W"
      },
      "source": [
        "\"\"\"\n",
        "Save dataframe with keywords and tf-idf scores into .csv file,\n",
        "using the same name (e.g. SanJose1.pdf => SanJose1.csv)\n",
        "\n",
        "keywords_df is result from function getKeywords()s\n",
        "\"\"\"\n",
        "def saveKeywordsAsCsv(filename, keywords_df):\n",
        "    # Create folder to store file\n",
        "    pathToFolder = \"/content/gdrive/My Drive/CFSJ/\"\n",
        "    os.makedirs(pathToFolder+\"Keywords-spaCy\", exist_ok=True)\n",
        "\n",
        "    # Create csv file name\n",
        "    name = filename[:-4]\n",
        "    csvFilename = name + \".csv\"\n",
        "\n",
        "    # Save file in folder\n",
        "    folder = \"Keywords-spaCy/\"\n",
        "    keywords_df.to_csv(pathToFolder + folder + csvFilename, index=False)\n",
        "    print(f\"Saved as {csvFilename}\")\n",
        "\n",
        "    return csvFilename"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOS9oIyOjcjr"
      },
      "source": [
        "<a name=\"save-as-txt\"></a>\n",
        "*** \n",
        "# Save Files\n",
        "  1. Save PDFs as txt files\n",
        "  2. Save keywords dataframe as csv files\n",
        "  3. Save common words as csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmhcEiAGkC06"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RJvMKS_jeDU"
      },
      "source": [
        "### Save PDFs as Txt Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb3E8WQOjjMM",
        "outputId": "b131776b-4184-4b10-f460-3d2f20828366"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/SanJoseFiles/\"\n",
        "\n",
        "print(\"Starting to convert all PDFs to txt files.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Saving file #{i+1}: {filename}\")\n",
        "    text = getText(pathToFile)\n",
        "    txtFile = savePDFAsTxt(filename, text)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(f\"Finished: Saved {i+1} txt files.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to convert all PDFs to txt files.\n",
            "--------------------------------------------------\n",
            "Saving file #1: SanJose19.pdf\n",
            "Saved as SanJose19.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #2: SanJose4.pdf\n",
            "Saved as SanJose4.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #3: SanJose13.pdf\n",
            "Saved as SanJose13.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #4: SanJose17.pdf\n",
            "Saved as SanJose17.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #5: SanJose11.pdf\n",
            "Saved as SanJose11.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #6: SanJose15.pdf\n",
            "Saved as SanJose15.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #7: SanJose9.pdf\n",
            "Saved as SanJose9.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #8: SanJose7.pdf\n",
            "Saved as SanJose7.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #9: SanJose5.pdf\n",
            "Saved as SanJose5.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #10: SanJose16.pdf\n",
            "Saved as SanJose16.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #11: SanJose14.pdf\n",
            "Saved as SanJose14.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #12: SanJose6.pdf\n",
            "Saved as SanJose6.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #13: SanJose8.pdf\n",
            "Saved as SanJose8.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #14: SanJose12.pdf\n",
            "Saved as SanJose12.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #15: SanJose10.pdf\n",
            "Saved as SanJose10.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #16: SanJose3.pdf\n",
            "Saved as SanJose3.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #17: SanJose18.pdf\n",
            "Saved as SanJose18.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Finished: Saved 17 txt files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFuJXcHEmOyr",
        "outputId": "fc803ffa-e104-426a-d621-4c2ce2043cf4"
      },
      "source": [
        "# SanJose1 and SanJose2\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/SanJoseFiles/\"\n",
        "\n",
        "filename1 = \"SanJose1.pdf\"\n",
        "text1 = getText(pathBeforeFile + filename1)\n",
        "txtFile1 = savePDFAsTxt(filename1, text1)\n",
        "\n",
        "filename2 = \"SanJose2.pdf\"\n",
        "text2 = getText(pathBeforeFile + filename2)\n",
        "txtFile2 = savePDFAsTxt(filename2, text2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as SanJose1.txt\n",
            "Saved as SanJose2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwYrwTXKjqbW"
      },
      "source": [
        "### Save Keywords as Csv Files\n",
        "\n",
        "- Requires CommonWords.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b_zKBx3jxJo",
        "outputId": "d8dc8080-f6b0-462e-9e01-f8577e5eeecb"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/SanJoseFiles/\"\n",
        "\n",
        "print(\"Starting to save keywords for all PDFs to csv files.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Saving file #{i+1}: {filename}\")\n",
        "    # Get keywords dataframe\n",
        "    keywords_df = getKeywords(pathToFile)\n",
        "    csvFile = saveKeywordsAsCsv(filename, keywords_df)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(f\"Finished: Saved {i+1} csv files.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to save keywords for all PDFs to csv files.\n",
            "--------------------------------------------------\n",
            "Saving file #1: SanJose19.pdf\n",
            "Saved as SanJose19.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #2: SanJose4.pdf\n",
            "Saved as SanJose4.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #3: SanJose13.pdf\n",
            "Saved as SanJose13.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #4: SanJose17.pdf\n",
            "Saved as SanJose17.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #5: SanJose11.pdf\n",
            "Saved as SanJose11.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #6: SanJose15.pdf\n",
            "Saved as SanJose15.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #7: SanJose9.pdf\n",
            "Saved as SanJose9.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #8: SanJose7.pdf\n",
            "Saved as SanJose7.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #9: SanJose5.pdf\n",
            "Saved as SanJose5.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #10: SanJose16.pdf\n",
            "Saved as SanJose16.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #11: SanJose14.pdf\n",
            "Saved as SanJose14.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #12: SanJose6.pdf\n",
            "Saved as SanJose6.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #13: SanJose8.pdf\n",
            "Saved as SanJose8.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #14: SanJose12.pdf\n",
            "Saved as SanJose12.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #15: SanJose10.pdf\n",
            "Saved as SanJose10.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #16: SanJose3.pdf\n",
            "Saved as SanJose3.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #17: SanJose18.pdf\n",
            "Saved as SanJose18.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #18: SanJose1.pdf\n",
            "Saved as SanJose1.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #19: SanJose2.pdf\n",
            "Saved as SanJose2.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Finished: Saved 19 csv files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4cV-_1ejyAW"
      },
      "source": [
        "### Save Common Words\n",
        "- Requires keywords .csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJq96Adwj2lb",
        "outputId": "ad21e8f3-fa93-4720-9f1b-70cba895a8e5"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/StopWords/\"\n",
        "os.makedirs(pathBeforeFile, exist_ok=True)\n",
        "df_commonWords = saveCommonWords(pathBeforeFile)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as CommonWords.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3YghvSSX_c"
      },
      "source": [
        "<a name=\"run-tests\"></a>\n",
        "*** \n",
        "# Run Tests\n",
        "(For memorandum and resolution files).\n",
        "  1. Test One File\n",
        "  2. Test All Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaFSekRa3FAI"
      },
      "source": [
        "### Test One File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzdGJjg5i7lw"
      },
      "source": [
        "Memorandum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ql4p9Wg8ac3i",
        "outputId": "21ed4d99-0220-4a27-8107-6ef44a381232"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "filename = \"Historic_Landmark_Designation_Property.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileTestResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75.0% of keywords were found.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>house</td>\n",
              "      <td>4.524365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>historic</td>\n",
              "      <td>4.650658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>property</td>\n",
              "      <td>4.670076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>contract</td>\n",
              "      <td>4.913023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>landmark</td>\n",
              "      <td>5.269697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>act</td>\n",
              "      <td>5.269697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>preservation</td>\n",
              "      <td>5.269697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mill</td>\n",
              "      <td>5.306065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>year</td>\n",
              "      <td>5.423848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>south</td>\n",
              "      <td>5.606170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Keywords    TF-IDF\n",
              "0         house  4.524365\n",
              "1      historic  4.650658\n",
              "2      property  4.670076\n",
              "3      contract  4.913023\n",
              "4      landmark  5.269697\n",
              "5           act  5.269697\n",
              "6  preservation  5.269697\n",
              "7          mill  5.306065\n",
              "8          year  5.423848\n",
              "9         south  5.606170"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk2u8-yNi_gX"
      },
      "source": [
        "Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "USCtTBGei-w6",
        "outputId": "4b410ef3-a169-4cfa-a9f7-920d643a8b13"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "filename = \"Fire_Department_Exam_Free_Use_Hall.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileTestResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0% of keywords were found.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>use</td>\n",
              "      <td>3.855032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>free</td>\n",
              "      <td>3.855032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fire</td>\n",
              "      <td>3.855032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>department</td>\n",
              "      <td>3.988564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saturday</td>\n",
              "      <td>4.142714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hall</td>\n",
              "      <td>4.325036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>august</td>\n",
              "      <td>4.325036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>authorize</td>\n",
              "      <td>4.325036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>process</td>\n",
              "      <td>4.548180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>exam</td>\n",
              "      <td>4.548180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Keywords    TF-IDF\n",
              "0         use  3.855032\n",
              "1        free  3.855032\n",
              "2        fire  3.855032\n",
              "3  department  3.988564\n",
              "4    saturday  4.142714\n",
              "5        hall  4.325036\n",
              "6      august  4.325036\n",
              "7   authorize  4.325036\n",
              "8     process  4.548180\n",
              "9        exam  4.548180"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyPf5yEvreqQ"
      },
      "source": [
        "### Test All Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J20LlPEHgZ3G"
      },
      "source": [
        "Memorandums"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElNUZ6Py5zGS",
        "outputId": "f53aadda-ee74-405f-e263-7e3c898753ee"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Downtown_Rezone_Addendum_Environmental.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Chief_Police_Questions_Policy_Selection.pdf\n",
            "60.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Dumpster_Day_Brooktree_Vinci_Flickinger.pdf\n",
            "40.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 4: Juneteenth_Holiday.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 5: Marriott_Townplace_Suites_Hotel_Vesting_Development_Permit.pdf\n",
            "14.29% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 6: Demolition_Permit_Site_Development_Construction_Building.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 7: Audit_Peer_Review.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 8: District_2_Great_Oaks_Movie_Night_Coffee_Chief.pdf\n",
            "12.5% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 9: Flag_Raising_University_San_Jose_State.pdf\n",
            "16.67% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 10: Pride_Flag_Silicon_Valley.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 11: Santos_Family_Car_Show.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 12: Vaccine_Mandate_Vaccination.pdf\n",
            "33.33% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 13: Trimble_Agnews_Groundwater_Wells_Project.pdf\n",
            "60.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 14: Historic_Landmark_Designation_Property.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QpdvTK1gb_q"
      },
      "source": [
        "Resolutions\n",
        "\n",
        "* Was not able to get text from \"Environmental_Mixed_Use_Construction.pdf\" (or SanJose16.pdf) with PyPDF2\n",
        "* just shows many newline chars\n",
        "\n",
        "\n",
        "* SanJose16.pdf works with pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QfNm0k4gdCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b72c89-5fb6-4fa3-9773-6c1aa63cecbf"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Financing_Commercial.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Fire_Department_Exam_Free_Use_Hall.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Vacate_Almaden_Property_Surplus.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 4: Environmental_Mixed_Use_Construction.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1XGg_2xutj"
      },
      "source": [
        "*** \n",
        "# Other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYGzZLtTiMAU"
      },
      "source": [
        "### View Common Words\n",
        "- Will change this later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETTZT_5X5Frj",
        "outputId": "e8accf7f-3cff-4a58-b23c-620b9c2944f3"
      },
      "source": [
        "df_commonWords = getCommonWords()\n",
        "df_commonWords[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['date',\n",
              " 'require',\n",
              " 'item',\n",
              " 'approve',\n",
              " 'adopt',\n",
              " 'provide',\n",
              " 'propose',\n",
              " 'project',\n",
              " 'service',\n",
              " 'follow',\n",
              " 'post',\n",
              " 'recommend',\n",
              " 'include',\n",
              " 'state',\n",
              " 'report',\n",
              " 'pursuant',\n",
              " 'government',\n",
              " 'receive',\n",
              " 'associate',\n",
              " 'use']"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "J2ssVB-4n_nK",
        "outputId": "3acc6214-0d2a-4fb0-b762-84a094af4fbd"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/StopWords/\"\n",
        "pd.read_csv(pathBeforeFile + \"CommonWords.csv\").head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>date</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>require</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>item</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>approve</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>adopt</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>provide</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>propose</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>project</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>service</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>follow</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>post</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>recommend</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>include</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>state</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>report</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word  Frequency\n",
              "0        date         16\n",
              "1     require         16\n",
              "2        item         16\n",
              "3     approve         16\n",
              "4       adopt         15\n",
              "5     provide         14\n",
              "6     propose         13\n",
              "7     project         13\n",
              "8     service         13\n",
              "9      follow         13\n",
              "10       post         13\n",
              "11  recommend         12\n",
              "12    include         12\n",
              "13      state         12\n",
              "14     report         12"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    }
  ]
}