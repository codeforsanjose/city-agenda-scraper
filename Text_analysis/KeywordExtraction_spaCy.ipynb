{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeywordExtraction_spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3mTYDKyR0N8"
      },
      "source": [
        "# Keyword Extraction with spaCy Processing\n",
        "***\n",
        "### Table of Contents \n",
        "\n",
        "  1. [Setup](#setup)\n",
        "\n",
        "  2. [Functions](#createfunctions)\n",
        "\n",
        "  3. [Run Tests](#run-tests)\n",
        "\n",
        "  4. [Save as Txt Files](#save-as-txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1knTe5Ol48jL"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "***\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQTBoh3R2I1"
      },
      "source": [
        "### Install Packages\n",
        "* PyPDF2\n",
        "* pdfplumber\n",
        "* spaCyPDFReader (cannot install)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SWdCRyRpbd",
        "outputId": "0547c7da-d4f0-4901-eb67-8a9649df83d9"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfcHT0RRZKhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a776fb35-c31e-4d11-e9f4-09f4af471c90"
      },
      "source": [
        "!pip install pdfplumber"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.7/dist-packages (0.5.28)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Requirement already satisfied: Wand in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (0.6.7)\n",
            "Requirement already satisfied: pdfminer.six==20200517 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (20200517)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.10.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PzXtmAkSPU8",
        "outputId": "c4e05767-b774-4eba-98d3-fc94461aef35"
      },
      "source": [
        "!pip install spacypdfreader"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement spacypdfreader (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for spacypdfreader\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoDLV33BR9_L"
      },
      "source": [
        "\n",
        "### Mount Google Drive\n",
        "* Files from three folders: Memorandums, Resolutions, SanJoseFiles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-aHUrePSGOj",
        "outputId": "61cabe81-5c4c-4c59-c3f1-dd2eeb4b797a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTDsoxg3Smo4"
      },
      "source": [
        "<a name=\"createfunctions\"></a>\n",
        "*** \n",
        "# Functions\n",
        "\n",
        "1.  Processing Text: load pdf text and create tokens\n",
        "    * getText_PyPDF2(pathToFile)\n",
        "    * getText(pathToFile)  **<---new**\n",
        "    * preprocessText(text)\n",
        "    * getStopWords( ) **<---updated**\n",
        "    * getParser( )\n",
        "    * processText(sentence, stopWords, parser)\n",
        "    * getSentences(text)\n",
        "    * getTokens(sentences)\n",
        "\n",
        "3.  TF-IDF: compute scores and get keywords\n",
        "    * get_tf_idf(tokens)\n",
        "    * getKeywords(pathToFile)\n",
        "  \n",
        "4.  Test: compare code's keywords with correct keywords\n",
        "    * getCorrectKeywords(pathToFile)\n",
        "    * testFilename(pathToFile)\n",
        "\n",
        "5. Save Files: save text as .txt, save keywords in .csv **<---new**\n",
        "    * saveAsTxt(filename, text)  **<---new**\n",
        "    * saveKeywordsAsCsv(filename, keywords_df) **<---new**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17v46NKdY5Ab"
      },
      "source": [
        "### 1. Processing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WOWJLMYebjn"
      },
      "source": [
        "Load text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TioxpRKZTwG_"
      },
      "source": [
        "\"\"\"\n",
        "Get raw text from pdf file as string using PyPDF2\n",
        "\"\"\"\n",
        "def getText_PyPDF2(pathToFile: str) -> str:\n",
        "    # Load pdf file\n",
        "    pdfFile = open(pathToFile, 'rb')\n",
        "    PDF_Reader = PyPDF2.PdfFileReader(pdfFile)\n",
        "\n",
        "    # Get total number of pages in document\n",
        "    numPages = PDF_Reader.getNumPages()\n",
        "    #print(f\"There are {numPages} pages in the file.\\n\")\n",
        "\n",
        "    # Combine text from all pages into one string\n",
        "    text = \"\"\n",
        "    for pg_number in range(numPages):\n",
        "      page = PDF_Reader.getPage(pg_number)\n",
        "      page_text = page.extractText()\n",
        "      text += page_text\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKvrFppdZbfm"
      },
      "source": [
        "\"\"\"\n",
        "Extract raw text from pdf using pdfplumber\n",
        "\n",
        "Problems with PyPDF2\n",
        "  - only extracted 2 of 13 pages for a file\n",
        "  - added many newline chars where they don't exist in the original \n",
        "\n",
        "Improvements with pdfplumber\n",
        "  - able to extract ALL text, including header, footer, image captions\n",
        "  - keeps general format of original pdf, just makes it all left-aligned\n",
        "  - all words extracted are as they appear in pdf \n",
        "    (significantly reduced the \"fake\" words)\n",
        "\n",
        "Initially found here: \n",
        "https://towardsdatascience.com/how-to-extract-text-from-pdf-245482a96de7\n",
        "\"\"\"\n",
        "def getText(pathToFile: str) -> str:\n",
        "    # Open pdf file\n",
        "    pdfFile = pdfplumber.open(pathToFile)\n",
        "\n",
        "    # Get list of all pages' objects\n",
        "    allPages = pdfFile.pages\n",
        "\n",
        "    # Extract text from each page and store into one string\n",
        "    allText = \"\"\n",
        "    for pageObject in allPages:\n",
        "        pageText = pageObject.extract_text()\n",
        "        allText += pageText\n",
        "    \n",
        "    return allText"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KgqjqJVOUaSm",
        "outputId": "7c55fc63-48a1-467d-abcd-d9b10359983b"
      },
      "source": [
        "\"\"\"\n",
        "Get text from pdf as a spacy doc object using spacypdfreader\n",
        "(spacypdfreader was not able to get installed)\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nGet text from pdf as a spacy doc object using spacypdfreader\\n(spacypdfreader was not able to get installed)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCilKovVZZx"
      },
      "source": [
        "Process Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk7fVH0aU8P1"
      },
      "source": [
        "\"\"\"\n",
        "Preprocess text by replacing newline with a space\n",
        "\"\"\"\n",
        "def preprocessText(text: str) -> str:\n",
        "    # Make lower case, and remove newline\n",
        "    preprocessedText = text.replace(\"\\n\", \" \")\n",
        "    return preprocessedText"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUILWbaRWVOU"
      },
      "source": [
        "\"\"\" \n",
        "Get list of stop words from spacy\n",
        "\"\"\"\n",
        "def getStopWords():\n",
        "  stopWords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  stopWordsList = list(stopWords) #was a set\n",
        "  stopWordsList += ['san', 'jose', 'josÃ©', 'city', 'council', \n",
        "                    'meeting', 'resolution', 'memorandum',\n",
        "                    'event', 'file', 'document', 'agenda',\n",
        "                    'draft', 'contact', 'office', 'resource',\n",
        "                    'clerk', 'final', 'california',]\n",
        "  return stopWordsList"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFOOwJNRW9_b"
      },
      "source": [
        "\"\"\"\n",
        "Create spacy Language object to parse English text\n",
        "\"\"\"\n",
        "def getParser():\n",
        "    parser = English()\n",
        "    return parser"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVSY1_bVbKF"
      },
      "source": [
        "\"\"\"\n",
        "Processing text: lemmatize, remove stop words, make lowercase\n",
        "Input:\n",
        "    sentence: str, \n",
        "    stopWords: set, \n",
        "    parser: spacy.lang.en.English\n",
        "\"\"\"\n",
        "\n",
        "def processText(sentence, stopWords, parser) -> list:\n",
        "    \n",
        "    # Create token object \n",
        "    tokens = parser(sentence)\n",
        "    \n",
        "    # Lemmatize each token and make them lower case\n",
        "    tokens_lemmatized = [word.lemma_ for word in tokens]\n",
        "    tokens_lowercase = [(word.lower().strip()) for word in tokens_lemmatized]\n",
        "    \n",
        "    # Removing stop words and any punctuation or numeric strings\n",
        "    list_tokens = []\n",
        "    for word in tokens_lowercase:\n",
        "        if word not in stopWords and word.isalpha():\n",
        "            list_tokens.append(word)\n",
        "    \n",
        "    # Remove single letters\n",
        "    list_tokens = [word for word in list_tokens if len(word)>1]\n",
        "    \n",
        "    # Return preprocessed list of tokens\n",
        "    return list_tokens  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUenlYFNhXya"
      },
      "source": [
        "\"\"\"\n",
        "Get sentences from pdf's preprocessed text using spacy's trained pipeline\n",
        "\"\"\"\n",
        "def getSentences(text: str) -> list:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    sentences = [sent.string.strip() for sent in sentences]\n",
        "    return sentences"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HG6t30RfdUc"
      },
      "source": [
        "\"\"\"\n",
        "Break sentences into words by processing them\n",
        "\"\"\"\n",
        "def getTokens(sentences: list) -> list:\n",
        "    stopWords = getStopWords()\n",
        "    parser = getParser()\n",
        "    # Process sentences to get words\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        current_tokens = processText(sentence, stopWords, parser)\n",
        "        tokens += current_tokens\n",
        "    # Return words with alphabet only\n",
        "    return tokens"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQO0773tX9pM"
      },
      "source": [
        "### 2. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkJ_OOuHX99J"
      },
      "source": [
        "\"\"\"\n",
        "Run tf-idf algorithm on the list of tokens, and\n",
        "return a dataframe with tokens and scores\n",
        "\"\"\"\n",
        "def get_TF_IDF(tokens: list):\n",
        "    \n",
        "    # compute TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_vectorizer.fit(tokens)\n",
        "\n",
        "    tf_idf = list(tfidf_vectorizer.idf_) #scores\n",
        "    features = list(tfidf_vectorizer.get_feature_names()) #words/tokens\n",
        "\n",
        "    # store results in dataframe\n",
        "    scores_df = pd.DataFrame(list(zip(features, tf_idf)), \n",
        "                             columns=['Keywords', 'TF-IDF'])\n",
        "    # sort by score in ascending - small to large - order\n",
        "    scores_df = scores_df.sort_values('TF-IDF').reset_index(drop=True)\n",
        "    \n",
        "    return scores_df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmxP3DxjeVH"
      },
      "source": [
        "\"\"\"\n",
        "Combine all functions to get keywords dataframe from just path to file\n",
        "\"\"\"\n",
        "def getKeywords(pathToFile: str): \n",
        "  rawText = getText(pathToFile=pathToFile)\n",
        "  preprocessedText = preprocessText(text = rawText)\n",
        "  sentences = getSentences(text = preprocessedText)\n",
        "  tokens = getTokens(sentences = sentences)\n",
        "\n",
        "  # get dataframe with keywords and scores\n",
        "  keywords_df = get_TF_IDF(tokens)\n",
        "\n",
        "  return keywords_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpRNQmKJl5Oa"
      },
      "source": [
        "### 3. Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEh8poeIluBG"
      },
      "source": [
        "\"\"\"\n",
        "Extract words from filename, \n",
        "which are separated by an underscore _\n",
        "\"\"\"\n",
        "def getCorrectKeywords(pathToFile: str) -> list:\n",
        "    filename = pathToFile.split('/')[-1]\n",
        "    correctKeywords = filename.replace(\".pdf\", \"\").split('_')\n",
        "    correctKeywords = [word.lower() for word in correctKeywords]\n",
        "    return correctKeywords"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7xX3Z0Il7KD"
      },
      "source": [
        "\"\"\"\n",
        "Test one file\n",
        "Compare correct keywords with the top 10 keywords computed with tf-idf\n",
        "\"\"\"\n",
        "def testFilename(pathToFile: str) -> float:\n",
        "    # get top 10 keywords using tf-idf\n",
        "    tf_idf_keywords = getKeywords(pathToFile)['Keywords'].to_list()[:10]\n",
        "\n",
        "    # get actual keywords from file name\n",
        "    correctKeywords = getCorrectKeywords(pathToFile)\n",
        "\n",
        "    numCorrectWordsFound = 0\n",
        "    for keyword in correctKeywords:\n",
        "        if keyword in tf_idf_keywords:\n",
        "            #print(f\"{keyword} was found.\")\n",
        "            numCorrectWordsFound += 1\n",
        "            \n",
        "        #else:\n",
        "            #print(f\"The word '{keyword}' was not found in tf-idf keywords.\")\n",
        "    \n",
        "    correctPercentage = round((numCorrectWordsFound/len(correctKeywords))*100, 2)\n",
        "    print(f\"{correctPercentage}% of keywords were found.\")\n",
        "\n",
        "    return correctPercentage"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMszvZGM7SyM"
      },
      "source": [
        "### 4. Save Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjZQb5707YvP"
      },
      "source": [
        "\"\"\"\n",
        "Save extracted text into .txt file,\n",
        "using the same name (e.g. SanJose1.pdf => SanJose1.txt)\n",
        "\"\"\"\n",
        "def savePDFAsTxt(filename: str, text: str):\n",
        "    # Create folder, if it doesn't exist \n",
        "    pathToFolder = \"/content/gdrive/My Drive/CFSJ/\"\n",
        "    os.makedirs(pathToFolder+\"TxtFiles\", exist_ok=True)\n",
        "\n",
        "    # Create txt file name\n",
        "    name = filename[:-4] #remove .pdf\n",
        "    txtFilename = name + \".txt\"\n",
        "\n",
        "    # Make txt file with name\n",
        "    txtFile = open(pathToFolder+\"TxtFiles/\"+txtFilename, 'w+')    \n",
        "    \n",
        "    # Write to file with string of pdf's text\n",
        "    txtFile.write(text.strip())\n",
        "    print(f\"Saved as {txtFilename}\")\n",
        "    return txtFile"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8dvnmP7d8W"
      },
      "source": [
        "\"\"\"\n",
        "Save dataframe with keywords and tf-idf scores into .csv file,\n",
        "using the same name (e.g. SanJose1.pdf => SanJose1.csv)\n",
        "\n",
        "keywords_df is result from function getKeywords()s\n",
        "\"\"\"\n",
        "def saveKeywordsAsCsv(filename, keywords_df):\n",
        "    # Create folder to store file\n",
        "    pathToFolder = \"/content/gdrive/My Drive/CFSJ/\"\n",
        "    os.makedirs(pathToFolder+\"Keywords-spaCy\", exist_ok=True)\n",
        "\n",
        "    # Create csv file name\n",
        "    name = filename[:-4]\n",
        "    csvFilename = name + \".csv\"\n",
        "\n",
        "    # Save file in folder\n",
        "    folder = \"Keywords-spaCy/\"\n",
        "    keywords_df.to_csv(pathToFolder + folder + csvFilename, index=False)\n",
        "    print(f\"Saved as {csvFilename}\")\n",
        "\n",
        "    return csvFilename"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3YghvSSX_c"
      },
      "source": [
        "<a name=\"run-tests\"></a>\n",
        "*** \n",
        "# Run Tests\n",
        "  1. Load libraries\n",
        "  2. Test 1 File\n",
        "  3. Test All Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgCU_9LhWBjU"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaFSekRa3FAI"
      },
      "source": [
        "### Test One File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzdGJjg5i7lw"
      },
      "source": [
        "Memorandum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Ql4p9Wg8ac3i",
        "outputId": "f96d74b8-94a5-4c28-c40b-805227a514f4"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "filename = \"Historic_Landmark_Designation_Property.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileTestResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75.0% of keywords were found.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>house</td>\n",
              "      <td>4.547931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>historic</td>\n",
              "      <td>4.657131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>property</td>\n",
              "      <td>4.847174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>contract</td>\n",
              "      <td>5.053026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>preservation</td>\n",
              "      <td>5.276170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>act</td>\n",
              "      <td>5.276170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mills</td>\n",
              "      <td>5.350278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>landmark</td>\n",
              "      <td>5.350278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>south</td>\n",
              "      <td>5.612642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>historical</td>\n",
              "      <td>5.663935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Keywords    TF-IDF\n",
              "0         house  4.547931\n",
              "1      historic  4.657131\n",
              "2      property  4.847174\n",
              "3      contract  5.053026\n",
              "4  preservation  5.276170\n",
              "5           act  5.276170\n",
              "6         mills  5.350278\n",
              "7      landmark  5.350278\n",
              "8         south  5.612642\n",
              "9    historical  5.663935"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk2u8-yNi_gX"
      },
      "source": [
        "Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "USCtTBGei-w6",
        "outputId": "1b3a3eb2-ea86-48a1-9dd7-7fece8fa05c9"
      },
      "source": [
        "# Run test for one file\n",
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "filename = \"Fire_Department_Exam_Free_Use_Hall.pdf\"\n",
        "pathToFile = pathBeforeFile + filename\n",
        "\n",
        "# Test\n",
        "fileTestResult = testFilename(pathToFile=pathToFile)\n",
        "\n",
        "# Top 10 keywords\n",
        "getKeywords(pathToFile).head(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0% of keywords were found.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keywords</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>use</td>\n",
              "      <td>3.883403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>free</td>\n",
              "      <td>3.883403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fire</td>\n",
              "      <td>3.883403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>saturday</td>\n",
              "      <td>4.171085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>department</td>\n",
              "      <td>4.171085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hall</td>\n",
              "      <td>4.353407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>august</td>\n",
              "      <td>4.353407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>process</td>\n",
              "      <td>4.576550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>exam</td>\n",
              "      <td>4.576550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>captain</td>\n",
              "      <td>4.576550</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Keywords    TF-IDF\n",
              "0         use  3.883403\n",
              "1        free  3.883403\n",
              "2        fire  3.883403\n",
              "3    saturday  4.171085\n",
              "4  department  4.171085\n",
              "5        hall  4.353407\n",
              "6      august  4.353407\n",
              "7     process  4.576550\n",
              "8        exam  4.576550\n",
              "9     captain  4.576550"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyPf5yEvreqQ"
      },
      "source": [
        "### Test All Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J20LlPEHgZ3G"
      },
      "source": [
        "Memorandums"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElNUZ6Py5zGS",
        "outputId": "273325a2-2b61-44bd-dda6-b84a8357ccca"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Memorandums/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Downtown_Rezone_Addendum_Environmental.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Chief_Police_Questions_Policy_Selection.pdf\n",
            "80.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Dumpster_Day_Brooktree_Vinci_Flickinger.pdf\n",
            "40.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 4: Juneteenth_Holiday.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 5: Marriott_Townplace_Suites_Hotel_Vesting_Development_Permit.pdf\n",
            "14.29% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 6: Demolition_Permit_Site_Development_Construction_Building.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 7: Audit_Peer_Review.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 8: District_2_Great_Oaks_Movie_Night_Coffee_Chief.pdf\n",
            "12.5% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 9: Flag_Raising_University_San_Jose_State.pdf\n",
            "33.33% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 10: Pride_Flag_Silicon_Valley.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 11: Santos_Family_Car_Show.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 12: Vaccine_Mandate_Vaccination.pdf\n",
            "33.33% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 13: Trimble_Agnews_Groundwater_Wells_Project.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 14: Historic_Landmark_Designation_Property.pdf\n",
            "75.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QpdvTK1gb_q"
      },
      "source": [
        "Resolutions\n",
        "\n",
        "* Was not able to get text from \"Environmental_Mixed_Use_Construction.pdf\" (or SanJose16.pdf) with PyPDF2\n",
        "* just shows many newline chars\n",
        "\n",
        "\n",
        "* SanJose16.pdf works with pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QfNm0k4gdCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe0abfc-5cc2-49cb-d53f-e552b8837e7b"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/Resolutions/\"\n",
        "\n",
        "print(\"Starting Tests.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Test {i+1}: {filename}\")\n",
        "    result = testFilename(pathToFile=pathToFile)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(\"Tests completed.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Tests.\n",
            "--------------------------------------------------\n",
            "Test 1: Financing_Commercial.pdf\n",
            "50.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 2: Fire_Department_Exam_Free_Use_Hall.pdf\n",
            "100.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 3: Vacate_Almaden_Property_Surplus.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Test 4: Environmental_Mixed_Use_Construction.pdf\n",
            "25.0% of keywords were found.\n",
            "\n",
            "--------------------------------------------------\n",
            "Tests completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6aXSOonyHHg"
      },
      "source": [
        "<a name=\"save-as-txt\"></a>\n",
        "*** \n",
        "# Save Files\n",
        "Save PDFs as txt files, and save keywords dataframe as csv files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj2FmCgRGVd_"
      },
      "source": [
        "### Save PDFs as Txt Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeTGMpnByT0y",
        "outputId": "b8c45af2-a58a-476e-f773-bed3e8f323b6"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/SanJoseFiles/\"\n",
        "\n",
        "print(\"Starting to convert all PDFs to txt files.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Saving file #{i+1}: {filename}\")\n",
        "    text = getText(pathToFile)\n",
        "    txtFile = savePDFAsTxt(filename, text)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(f\"Finished: Saved {i+1} txt files.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to convert all PDFs to txt files.\n",
            "--------------------------------------------------\n",
            "Saving file #1: SanJose19.pdf\n",
            "Saved as SanJose19.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #2: SanJose4.pdf\n",
            "Saved as SanJose4.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #3: SanJose13.pdf\n",
            "Saved as SanJose13.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #4: SanJose17.pdf\n",
            "Saved as SanJose17.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #5: SanJose11.pdf\n",
            "Saved as SanJose11.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #6: SanJose15.pdf\n",
            "Saved as SanJose15.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #7: SanJose9.pdf\n",
            "Saved as SanJose9.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #8: SanJose7.pdf\n",
            "Saved as SanJose7.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #9: SanJose5.pdf\n",
            "Saved as SanJose5.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #10: SanJose16.pdf\n",
            "Saved as SanJose16.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #11: SanJose14.pdf\n",
            "Saved as SanJose14.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #12: SanJose6.pdf\n",
            "Saved as SanJose6.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #13: SanJose8.pdf\n",
            "Saved as SanJose8.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #14: SanJose12.pdf\n",
            "Saved as SanJose12.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #15: SanJose10.pdf\n",
            "Saved as SanJose10.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #16: SanJose3.pdf\n",
            "Saved as SanJose3.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #17: SanJose18.pdf\n",
            "Saved as SanJose18.txt\n",
            "\n",
            "--------------------------------------------------\n",
            "Finished: Saved 17 txt files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjFtPBh7GYsu"
      },
      "source": [
        "### Save Keywords as Csv Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIPTe5YqzWuE",
        "outputId": "6d0d1f4e-0df9-4939-dc21-2a4b07df56d2"
      },
      "source": [
        "pathBeforeFile = \"/content/gdrive/My Drive/CFSJ/SanJoseFiles/\"\n",
        "\n",
        "print(\"Starting to save keywords for all PDFs to csv files.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "for i,filename in enumerate(os.listdir(pathBeforeFile)):\n",
        "    pathToFile = pathBeforeFile + filename\n",
        "    print(f\"Saving file #{i+1}: {filename}\")\n",
        "    # Get keywords dataframe\n",
        "    keywords_df = getKeywords(pathToFile)\n",
        "    csvFile = saveKeywordsAsCsv(filename, keywords_df)\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "print(f\"Finised: Saved {i+1} csv files.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to save keywords for all PDFs to csv files.\n",
            "--------------------------------------------------\n",
            "Saving file #1: SanJose19.pdf\n",
            "Saved as SanJose19.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #2: SanJose4.pdf\n",
            "Saved as SanJose4.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #3: SanJose13.pdf\n",
            "Saved as SanJose13.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #4: SanJose17.pdf\n",
            "Saved as SanJose17.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #5: SanJose11.pdf\n",
            "Saved as SanJose11.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #6: SanJose15.pdf\n",
            "Saved as SanJose15.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #7: SanJose9.pdf\n",
            "Saved as SanJose9.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #8: SanJose7.pdf\n",
            "Saved as SanJose7.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #9: SanJose5.pdf\n",
            "Saved as SanJose5.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #10: SanJose16.pdf\n",
            "Saved as SanJose16.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #11: SanJose14.pdf\n",
            "Saved as SanJose14.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #12: SanJose6.pdf\n",
            "Saved as SanJose6.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #13: SanJose8.pdf\n",
            "Saved as SanJose8.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #14: SanJose12.pdf\n",
            "Saved as SanJose12.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #15: SanJose10.pdf\n",
            "Saved as SanJose10.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #16: SanJose3.pdf\n",
            "Saved as SanJose3.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Saving file #17: SanJose18.pdf\n",
            "Saved as SanJose18.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "Finised: Saved 17 csv files.\n"
          ]
        }
      ]
    }
  ]
}